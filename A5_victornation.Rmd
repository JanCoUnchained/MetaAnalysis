---
title: "Assignment 5 - Meta-analysis of pitch in schizophrenia"
author: "Riccardo Fusaroli"
date: "3/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Building on the shoulders of giants: meta-analysis

## Questions to be answered

1. What is the current evidence for distinctive patterns of pitch mean and pitch sd in schizophrenia? Report how many papers report quantitative estimates, your method to analyze them, the estimated effect size of the difference (mean effect size and standard error for pitch mean, same for pitch sd) and forest plots representing it. 

2. Do the results match your own analysis from Assignment 3? If you add your results to the meta-analysis, do the estimated effect sizes change? Report the new estimates and the new forest plots.

3. Assess the quality of the literature: report and comment on heterogeneity of the studies (tau, I2), on publication bias (funnel plot), and on influential studies.

## Tips on the process to follow:

- Download the data on all published articles analyzing pitch in schizophrenia (on gitlab)
- Look through the dataset to find out which columns to use, and if there is any additional information written as comments (real world data is always messy!).
    * Hint: Make sure you read the comments in the columns: `pitch_f0_variability`, `frequency`, `Title`,  `ACOUST_ANA_DESCR`, `DESCRIPTION`, and `COMMENTS`
- Following the procedure in the slides calculate effect size and standard error of the effect size per each study. N.B. we focus on pitch mean and pitch standard deviation.
 . first try using lmer (to connect to what you know of mixed effects models)
 . then use rma() (to get some juicy additional statistics)

- Build a forest plot of the results (forest(model))
 
- Go back to Assignment 3, add your own study to the data table, and re-run meta-analysis. Do the results change?

- Now look at the output of rma() and check tau and I2

```{r}
#STOLEN STRAIGHT FROM MIKKEL, lOl

library(pacman)



p_load(metafor, tidyverse, caret, lmerTest)

data <- read.csv("Assignment5_MetaAnalysis_SR_SCHIZO.csv")

PitchRange=escalc('SMD', n1i=SAMPLE_SIZE_SZ, n2i=SAMPLE_SIZE_HC, m1i=PITCH_F0_SZ_M,
m2i=PITCH_F0_HC_M, sd1i=PITCH_F0_SZ_SD, sd2i=PITCH_F0_HC_SD,
data = data)

PitchRangeSD=escalc('SMD', n1i=SAMPLE_SIZE_SZ, n2i=SAMPLE_SIZE_HC, m1i=PITCH_F0SD_SZ_M,
m2i=PITCH_F0SD_HC_M, sd1i=PITCH_F0SD_SZ_SD, sd2i=PITCH_F0SD_HC_SD,
data = data)
PitchRange <- filter(PitchRange, complete.cases(PitchRange$yi))
PitchRangeSD <- filter(PitchRangeSD, complete.cases(PitchRangeSD$yi))

colnames(PitchRangeSD)[22] <- "yiSD"
colnames(PitchRangeSD)[23] <- "viSD"

meta <- dplyr::full_join(PitchRange, PitchRangeSD)

#deleting useless columns 
```

partialing up into two datasets 

```{r}

mean_analysis <- meta %>%
  filter(yi != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0_SZ_M, PITCH_F0_HC_M, PITCH_F0_SZ_SD, PITCH_F0_HC_SD, yi, vi)

sd_analysis <- meta %>%
  filter(yiSD != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0SD_SZ_M, PITCH_F0SD_HC_M, PITCH_F0SD_SZ_SD, PITCH_F0SD_HC_SD, yiSD, viSD)

```

Model for Mean
--> Effect size around 0.5 - but the SE is very big. 
--> No conclusive evidence for an effect of mean pitch. 

```{r}
library(metafor)
m1_mean <- lmer(yi ~ 1 + (1|StudyID), weights = 1/vi, data=mean_analysis,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_mean <- rma(yi, vi, data = mean_analysis, slab=Article)
forest(m2_mean)

```

Model for SD 
--> also crosses zero, so no conclusive evidence. 

```{r}

m1_SD <- lmer(yiSD ~ 1 + (1|StudyID), weights = 1/viSD, data=sd_analysis,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_SD <- rma(yiSD, viSD, data = sd_analysis, slab=Article)
forest(m2_SD)

```

Quality check - Heterogeneity
--> Very high heterogeneity. 
--> A lot that is not due to chance. 

```{r}

confint(m2_mean, level = .95) #high
confint(m2_SD, level = .95) #extreme

```

Influential studies. 

COHEN: 
SD in every utterance not across. 
Figure out why it is very different (is it studying something else).
--> if not: multiverse analysis --> do the analysis without Cohen (does this change anything). 


```{r}

#mean
inf <- influence(m2_mean)
print(inf)
plot(inf) #study 2 is fucked. 

#sd
inf1 <- influence(m2_SD)
print(inf1)
plot(inf1) #study 6 is fucked. 

```


Funnelplots - publication bias (needs quantification)

```{r}

funnel(m2_mean, main = "Random-Effects Model", xlab = "Standardized Mean
Difference")

funnel(m2_SD, main = "Random-Effects Model", xlab = "Standardized Mean
Difference")

```

#### TASK 3 ####

Loading in our data? 

```{r}

#STUDY ID, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, 
our_data <- read.csv("ass3_studies.csv")
our_data <- our_data %>%
  mutate(StudyID = c(49,50,51)) %>%
  mutate(Article = c("JanCo et al. (3000)", "JanCo more al. (3000)", "JanCo most al. (3000)"))

#crazy try
rbind.match.columns <- function(input1, input2) {
    n.input1 <- ncol(input1)
    n.input2 <- ncol(input2)
 
    if (n.input2 < n.input1) {
        TF.names <- which(names(input2) %in% names(input1))
        column.names <- names(input2[, TF.names])
    } else {
        TF.names <- which(names(input1) %in% names(input2))
        column.names <- names(input1[, TF.names])
    }
 
    return(rbind(input1[, column.names], input2[, column.names]))
}
 
#joining them. 
withoutwithout <- rbind.match.columns(our_data, meta)
```

two dataframes with own. 

```{r}
mean_JanCo <- withoutwithout %>%
  filter(yi != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0_SZ_M, PITCH_F0_HC_M, PITCH_F0_SZ_SD, PITCH_F0_HC_SD, yi, vi)

sd_JanCo <- withoutwithout %>%
  filter(yiSD != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0SD_SZ_M, PITCH_F0SD_HC_M, PITCH_F0SD_SZ_SD, PITCH_F0SD_HC_SD, yiSD, viSD)

```

running analysis on mean 
--> now significant predictor. 

```{r}

m1_MJanCo <- lmer(yi ~ 1 + (1|StudyID), weights = 1/vi, data=mean_JanCo,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_MJanCo <- rma(yi, vi, data = mean_JanCo, slab=Article)
forest(m2_MJanCo)

```

running analysis on sd
--> still not significant (fucking outlier)

```{r}
m1_SDJanCo <- lmer(yiSD ~ 1 + (1|StudyID), weights = 1/viSD, data=sd_JanCo,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_SDJanCo <- rma(yiSD, viSD, data = sd_JanCo, slab=Article)
forest(m2_SDJanCo)

```

How fuck is everything?

```{r}

confint(m2_MJanCo, level = .95) #interpret. 
confint(m2_SDJanCo, level = .95) #interpret. 

```

Influential studies:
--> of course there is still one fucking bitch study (Cohen)

```{r}
#mean
inf <- influence(m2_MJanCo)
print(inf)
plot(inf) #study 2 is fucked. 

#sd
inf1 <- influence(m2_SDJanCo)
print(inf1)
plot(inf1) #study 6 is fucked. 
```

Funnel plots

```{r}
funnel(m2_MJanCo, main = "Random-Effects Model", xlab = "Standardized Mean
Difference") #something lacking to the left..?

funnel(m2_SDJanCo, main = "Random-Effects Model", xlab = "Standardized Mean
Difference") #crazy outlier - difficult to interpret. 
```

#### WITHOUT US, WITHOUT COHEN #### 

excluding Cohen 

```{r}
no_JanCohen <- meta %>%
  filter(!StudyID %in% c(15))
```

NB: I have not changed variable names for this analysis. 

```{r}

mean_analysis <- no_JanCohen %>%
  filter(yi != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0_SZ_M, PITCH_F0_HC_M, PITCH_F0_SZ_SD, PITCH_F0_HC_SD, yi, vi)

sd_analysis <- no_JanCohen %>%
  filter(yiSD != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0SD_SZ_M, PITCH_F0SD_HC_M, PITCH_F0SD_SZ_SD, PITCH_F0SD_HC_SD, yiSD, viSD)

```

Model for Mean:

```{r}
library(metafor)
m1_mean <- lmer(yi ~ 1 + (1|StudyID), weights = 1/vi, data=mean_analysis,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_mean <- rma(yi, vi, data = mean_analysis, slab=Article)
forest(m2_mean)

```

Model for SD:
--> significant. 

```{r}

m1_SD <- lmer(yiSD ~ 1 + (1|StudyID), weights = 1/viSD, data=sd_analysis,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_SD <- rma(yiSD, viSD, data = sd_analysis, slab=Article)
forest(m2_SD)

```

Quality check - Heterogeneity:

```{r}

confint(m2_mean, level = .95) #high
confint(m2_SD, level = .95) #extreme

```

Influential studies. 

COHEN: 
SD in every utterance not across. 
Figure out why it is very different (is it studying something else).
--> if not: multiverse analysis --> do the analysis without Cohen (does this change anything). 


```{r}

#mean
inf <- influence(m2_mean)
print(inf)
plot(inf) #study 2 is fucked. 

#sd
inf1 <- influence(m2_SD)
print(inf1)
plot(inf1) #study 6 is fucked. 

```


Funnelplots - publication bias (needs quantification)

```{r}

funnel(m2_mean, main = "Random-Effects Model", xlab = "Standardized Mean
Difference")

funnel(m2_SD, main = "Random-Effects Model", xlab = "Standardized Mean
Difference")

```


#### WITH US, WITHOUT COHEN #### 

Excluding Cohen

```{r}

no_cohen <- withoutwithout %>%
  filter(!StudyID %in% c(15))

```

two models without cohen

```{r}
mean_no_cohen <- no_cohen %>%
  filter(yi != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0_SZ_M, PITCH_F0_HC_M, PITCH_F0_SZ_SD, PITCH_F0_HC_SD, yi, vi)

sd_no_cohen <- no_cohen %>%
  filter(yiSD != 'NA') %>%
  select(StudyID, Article, SAMPLE_SIZE_SZ, SAMPLE_SIZE_HC, PITCH_F0SD_SZ_M, PITCH_F0SD_HC_M, PITCH_F0SD_SZ_SD, PITCH_F0SD_HC_SD, yiSD, viSD)

```

Model for Mean
--> surprise, surprise. Also significant without Cohen. 

```{r}
library(metafor)
m1_mean <- lmer(yi ~ 1 + (1|StudyID), weights = 1/vi, data=mean_no_cohen,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_mean <- rma(yi, vi, data = mean_no_cohen, slab=Article)
forest(m2_mean)

```

Model for SD 
--> significant without Cohen (fuck that guy) :) :) 

```{r}

m1_SD <- lmer(yiSD ~ 1 + (1|StudyID), weights = 1/viSD, data=sd_no_cohen,
control=lmerControl(check.nobs.vs.nlev="ignore",
check.nobs.vs.nRE="ignore"))

m2_SD <- rma(yiSD, viSD, data = sd_no_cohen, slab=Article)
forest(m2_SD)

```

Quality check - Heterogeneity
--> Very high heterogeneity. 
--> A lot that is not due to chance. 

```{r}

confint(m2_mean, level = .95) #high
confint(m2_SD, level = .95) #extreme

```

Influential studies. 

COHEN: 
SD in every utterance not across. 
Figure out why it is very different (is it studying something else).
--> if not: multiverse analysis --> do the analysis without Cohen (does this change anything). 


```{r}

#mean
inf <- influence(m2_mean)
print(inf)
plot(inf) #study 2 is fucked. 

#sd
inf1 <- influence(m2_SD)
print(inf1)
plot(inf1) #study 6 is fucked. 

```

Funnelplots - publication bias (needs quantification)

```{r}

funnel(m2_mean, main = "Random-Effects Model", xlab = "Standardized Mean
Difference") #still something lacking left with big SE.  

funnel(m2_SD, main = "Random-Effects Model", xlab = "Standardized Mean
Difference") #looks terrible. 

```
